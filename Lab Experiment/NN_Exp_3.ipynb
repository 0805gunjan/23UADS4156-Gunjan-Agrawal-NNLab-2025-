{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WAP to implement a three-layer neural network using Tensor flow library (only, no keras) to classify MNIST handwritten digits dataset. Demonstrate the implementation of feed-forward and back-propagation approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of the code\n",
    "\n",
    "* Tensorflow library provides interface to artificial neural network.\n",
    "* The MNIST dataset is loaded.\n",
    "* Feature engineering is done as normalization.\n",
    "* An input layer with 784 neurons (flattened 28x28 images)\n",
    "* Two hidden layers with 128 and 64 neurons, using Sigmoid activation function.\n",
    "* An output layer with 10 neurons (corresponding to digit classes).\n",
    "* Epoch: 20 is used.\n",
    "* Epoch: If the model keeps improving, it is advisable to try a higher number of epochs. If the model stopped improving way before the final epoch, it is advisable to try a lower number of epochs.\n",
    "* Batch size - 100 \n",
    "* Batch size refers to the number of samples used in one iteration.\n",
    "* Optimization via Adam optimizer to minimize loss.\n",
    "* Loss function: Softmax cross entropy is used "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.1419, Train Accuracy: 95.72, Test Accuracy: 95.16\n",
      "Epoch 2, Loss: 0.0987, Train Accuracy: 96.92, Test Accuracy: 96.10\n",
      "Epoch 3, Loss: 0.0815, Train Accuracy: 97.38, Test Accuracy: 96.27\n",
      "Epoch 4, Loss: 0.0734, Train Accuracy: 97.62, Test Accuracy: 96.44\n",
      "Epoch 5, Loss: 0.0547, Train Accuracy: 98.25, Test Accuracy: 96.91\n",
      "Epoch 6, Loss: 0.0694, Train Accuracy: 97.72, Test Accuracy: 96.59\n",
      "Epoch 7, Loss: 0.0646, Train Accuracy: 97.91, Test Accuracy: 96.49\n",
      "Epoch 8, Loss: 0.0411, Train Accuracy: 98.66, Test Accuracy: 97.03\n",
      "Epoch 9, Loss: 0.0504, Train Accuracy: 98.32, Test Accuracy: 96.84\n",
      "Epoch 10, Loss: 0.0402, Train Accuracy: 98.68, Test Accuracy: 97.08\n",
      "Epoch 11, Loss: 0.0355, Train Accuracy: 98.78, Test Accuracy: 97.21\n",
      "Epoch 12, Loss: 0.0326, Train Accuracy: 98.93, Test Accuracy: 97.09\n",
      "Epoch 13, Loss: 0.0320, Train Accuracy: 98.92, Test Accuracy: 97.07\n",
      "Epoch 14, Loss: 0.0304, Train Accuracy: 98.99, Test Accuracy: 97.24\n",
      "Epoch 15, Loss: 0.0207, Train Accuracy: 99.34, Test Accuracy: 97.51\n",
      "Epoch 16, Loss: 0.0306, Train Accuracy: 98.93, Test Accuracy: 97.04\n",
      "Epoch 17, Loss: 0.0405, Train Accuracy: 98.68, Test Accuracy: 96.82\n",
      "Epoch 18, Loss: 0.0216, Train Accuracy: 99.29, Test Accuracy: 97.59\n",
      "Epoch 19, Loss: 0.0285, Train Accuracy: 99.08, Test Accuracy: 97.33\n",
      "Epoch 20, Loss: 0.0266, Train Accuracy: 99.11, Test Accuracy: 97.40\n",
      "Final Train Accuracy: 99.11\n",
      "Final Test Accuracy: 97.40\n",
      "Training Complete!\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.python.framework.ops import disable_eager_execution\n",
    "\n",
    "disable_eager_execution()  # Disable eager execution to use TensorFlow's graph execution\n",
    "\n",
    "# Load MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train.reshape(-1, 784) / 255.0, x_test.reshape(-1, 784) / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = np.eye(10)[y_train]\n",
    "y_test = np.eye(10)[y_test]\n",
    "\n",
    "# Define model hyperparameters\n",
    "input_size = 784\n",
    "hidden1_size = 128\n",
    "hidden2_size = 64\n",
    "output_size = 10\n",
    "learning_rate = 0.01\n",
    "batch_size = 100\n",
    "epochs = 20\n",
    "\n",
    "# Define placeholders for input and output\n",
    "X = tf.compat.v1.placeholder(tf.float32, [None, input_size])\n",
    "y = tf.compat.v1.placeholder(tf.float32, [None, output_size])\n",
    "\n",
    "# Initialize weights and biases\n",
    "weights = {\n",
    "    'w1': tf.Variable(tf.random.truncated_normal([input_size, hidden1_size], stddev=0.1)),\n",
    "    'w2': tf.Variable(tf.random.truncated_normal([hidden1_size, hidden2_size], stddev=0.1)),\n",
    "    'w3': tf.Variable(tf.random.truncated_normal([hidden2_size, output_size], stddev=0.1))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.zeros([hidden1_size])),\n",
    "    'b2': tf.Variable(tf.zeros([hidden2_size])),\n",
    "    'b3': tf.Variable(tf.zeros([output_size]))\n",
    "}\n",
    "\n",
    "# Define feed-forward neural network\n",
    "def neural_network(X):\n",
    "    layer1 = tf.nn.sigmoid(tf.matmul(X, weights['w1']) + biases['b1'])\n",
    "    layer2 = tf.nn.sigmoid(tf.matmul(layer1, weights['w2']) + biases['b2'])\n",
    "    output_layer = tf.matmul(layer2, weights['w3']) + biases['b3']\n",
    "    return output_layer\n",
    "\n",
    "# Compute logits\n",
    "logits = neural_network(X)\n",
    "\n",
    "# Define loss function (cross-entropy)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=logits))\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "# Define accuracy metric\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Run session\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "with tf.compat.v1.Session() as sess:\n",
    "    sess.run(tf.compat.v1.global_variables_initializer())\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, len(x_train), batch_size):\n",
    "            batch_x, batch_y = x_train[i:i+batch_size], y_train[i:i+batch_size]\n",
    "            sess.run(optimizer, feed_dict={X: batch_x, y: batch_y})\n",
    "        \n",
    "        # Calculate and display loss and accuracy at each epoch\n",
    "        train_loss, train_acc = sess.run([loss, accuracy], feed_dict={X: x_train, y: y_train})\n",
    "        test_acc = sess.run(accuracy, feed_dict={X: x_test, y: y_test})\n",
    "        print(f\"Epoch {epoch+1}, Loss: {train_loss:.4f}, Train Accuracy: {train_acc*100:.2f}, Test Accuracy: {test_acc*100:.2f}\")\n",
    "    \n",
    "    # Compute final train and test accuracy\n",
    "    final_train_acc = sess.run(accuracy, feed_dict={X: x_train, y: y_train})\n",
    "    final_test_acc = sess.run(accuracy, feed_dict={X: x_test, y: y_test})\n",
    "    print(f\"Final Train Accuracy: {final_train_acc*100:.2f}\")\n",
    "    print(f\"Final Test Accuracy: {final_test_acc*100:.2f}\")\n",
    "    \n",
    "    print(\"Training Complete!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description of the code\n",
    "\n",
    "* Imported numpy and tensorflow.\n",
    "* Disabled the tensorflow 2.x by disable eager execution as we are using tensorflow 1.x. \n",
    "\n",
    " **Data Preprocessing**:\n",
    "   - Loads the MNIST dataset.\n",
    "   - Normalizes pixel values range 0 to 1 so that input values are small, model can perform better.\n",
    "   - Reshape images into vectors and convert labels into one-hot encoding.\n",
    "\n",
    " **Network Initialization**:\n",
    "   - Defines the structure with two hidden layers.\n",
    "   - Initializes weights and biases.\n",
    "      * W1, W2, b1 and b2 are weights and biases of the hidden layer.\n",
    "      * W3 and b3 are weights and biases of the output layer.\n",
    "      * Weights are initialized randomly between -1 and 1. \n",
    "      * Biases are initialized to zeroes.\n",
    "   - Placeholder defines the input and output values storage as where it is stored like it can be stored in RAM or in GPU or in Cache\n",
    "  \n",
    "  **Feed-Forward Neural Network**\n",
    "   - First and second hidden layer uses sigmoid activation function.\n",
    "   - Output layer uses softmax during loss calculation and does not use any activation function.\n",
    "   - Matmul function is for matrix multiplication.\n",
    "\n",
    "  **Compute Logits**\n",
    "   - Passes X through the network to compute predictions (logits).\n",
    "   - Computes logits (raw scores before softmax).\n",
    "   - Final logits are passed to the softmax cross-entropy loss function.\n",
    "\n",
    "  **Backpropagation & Optimization**\n",
    "   - Uses Adam optimizer to minimize the cross-entropy loss.\n",
    "   - Gradients are computed and used to adjust weights and biases.\n",
    "\n",
    "  **Accuracy Metric**\n",
    "   - argmax(logits, 1): Gets the predicted class.\n",
    "   - argmax(y, 1): Gets the actual class.\n",
    "   - equal(): Compares predicted vs actual class.\n",
    "   - reduce_mean(): Computes the accuracy.\n",
    "\n",
    "  **Session**\n",
    "   - It runs session on the global variable initializer.\n",
    "\n",
    "  **Training of the model**\n",
    "   * In session, training of the model is done by iterating the training data through batches.\n",
    "   * For n iterations, n epochs is done and for batch size, there is 100 batch size that means as for every 100 feedforward there is one backpropagation.\n",
    "   * Prints loss and accuracy after each epoch.\n",
    "   * Evaluates final accuracy on both training and test data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Evaluation \n",
    "* The model achieves high accuracy (~99% on training and ~97% on test data).\n",
    "* The performance is satisfactory for MNIST classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Comments(Limitations and Improvements)\n",
    " * Sigmoid activation is not ideal for deep networks due to the vanishing gradient problem, which slows down learning.\n",
    " * This code uses TensorFlow 1.x style graph execution, which requires disabling eager execution.\n",
    "\n",
    "#### Improvements\n",
    " * Use TensorFlow 2.x with Keras Functional API for modern implementation.\n",
    " * Use of ReLu activation function for speeding up training and gradient flow."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
