{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4abec90",
   "metadata": {},
   "source": [
    "### WAP to evaluate the performance of implemented three-layer neural network with variations in activation functions, size of hidden layer, learning rate, batch size and number of epochs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c931dc",
   "metadata": {},
   "source": [
    "#### Description of the Model\n",
    "\n",
    "* The MNIST dataset is loaded using tensorflow library\n",
    "* In the model, the architecture is customizable in which different activation functions and hidden layer sizes, providing flexibility for experimentation.\n",
    "* The activation functions are Sigmoid, Relu, Tanh.\n",
    "* the hidden layer varies as 256, 128 and 64.\n",
    "* An input layer with 784 neurons(flattened 28*28 pixels).\n",
    "* Epoch: 50 is used\n",
    "* Learning rate: 0.1 is taken\n",
    "* An output layer of 10 neurons.\n",
    "* Batch size: 10\n",
    "* Sigmoid for probabilistic interpretation.\n",
    "* Tanh for symmetric output between -1 and 1.\n",
    "* ReLU for sparse activations and faster training.\n",
    "* Optimizer : SGD(Stochastic Gradient Descent) is used to minimize the loss function by adjusting model parameters using gradients.\n",
    "* A Confusion Matrix is plotted using Seaborn to visualize classification performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9d1b0b",
   "metadata": {},
   "source": [
    "#### Description of the code\n",
    "\n",
    "* Import of libraries\n",
    "    - TensorFlow for building and training the neural network.\n",
    "    - NumPy for numerical operations.\n",
    "    - Matplotlib for plotting graphs.\n",
    "    - Seaborn for creating a heatmap visualization of the confusion matrix.\n",
    "    - Sklearn for calculating the confusion matrix.\n",
    "\n",
    "* Load the dataset\n",
    "    - The MNIST dataset is loaded using tf.keras.datasets.mnist.load_data().\n",
    "    - Data is reshaped from 28x28 to 784-dimensional vectors.\n",
    "    - The pixel values are normalized to the range [0, 1] for faster convergence.\n",
    "\n",
    "**Network Initialization**:\n",
    "   - Defines the structure with hidden layers.\n",
    "   - Initializes weights and biases.\n",
    "      * W1, W2, b1 and b2 are weights and biases of the hidden layer.\n",
    "      * W3 and b3 are weights and biases of the output layer.\n",
    "      * Weights are initialized randomly between -1 and 1. \n",
    "\n",
    "**Forward pass**\n",
    "* The input is passed through the network with: Z1 = XW1+b1, A1= activation(Z1), Z2 = A1W2 + b2\n",
    "* The output logits are produced without applying softmax. Softmax is applied later for loss computation.\n",
    "* The activation function (e.g., Sigmoid,ReLU or Tanh) is applied to the hidden layer.\n",
    "\n",
    "**Loss Function**\n",
    "* The loss function used is Softmax Cross-Entropy. It measures the difference between predicted logits and actual labels.\n",
    "* Stochastic Gradient Descent (SGD) is used to minimize the loss by adjusting the model parameters.\n",
    "\n",
    "**Training Model**\n",
    "* Gradient Tape:It allows you to compute gradients of a loss function with respect to model parameters like weights and biases.\n",
    "* The model trains for 50 epochs, computing the average loss for each epoch using mini-batches.\n",
    "* The loss value is stored for visualization later.\n",
    "* Execution time is calculated over the training loop.\n",
    "* After each epoch, predictions are made using the training data.\n",
    "* Accuracy is calculated by comparing predictions to actual labels.\n",
    "\n",
    "Loss Curve, Confusion matrix and accuracy curve is plotted.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d4e1b3",
   "metadata": {},
   "source": [
    "#### Performance Evaluation\n",
    "\n",
    "* Performance has been evaluated by plotting Loss curve , Training Accuracy\n",
    "curve and Confusion matrix.\n",
    "* The test accuracy for Network architecture 1 is Test Accuracy: 98.35% as it contains a single hidden layer 256 and\n",
    "activation function is ReLU , it has executed in least time(2990.97seconds).\n",
    "* The test accuracy for Network architecture has 9 configurations vary according to the model parameters.\n",
    "* The test accoracy is maximum for the hidden layer 256. As the size of the hidden layer decreases accuracy also decreases.\n",
    "* The test accuracy varies between 97-98% for the activation function.\n",
    "* The maximum exection time taken is by Tanh activation function(hidden layer 256) is *5564.17sec* and the minimum time taken is by sigmoid activation function (hidden layer 64) is *2617.29sec*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be271ecb",
   "metadata": {},
   "source": [
    "#### My comments(Limitations and Scope for improvemnet)\n",
    "\n",
    "* Maximum test accuracy reached are Test Accuracy: 98.35% for configuration of hidden layer 256 and activation function is ReLU.\n",
    "* To improve accuracy and decrease execution time , a different activation\n",
    "function can be used , batch size can be increased , the learning rate value can\n",
    "be changed , the number of hidden layers and number of hidden layer neurons\n",
    "need to be changed ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
